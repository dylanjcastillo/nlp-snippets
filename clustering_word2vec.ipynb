{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dylancastillo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\"data/news_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>url_to_image</th>\n",
       "      <th>published_at</th>\n",
       "      <th>content</th>\n",
       "      <th>top_article</th>\n",
       "      <th>engagement_reaction_count</th>\n",
       "      <th>engagement_comment_count</th>\n",
       "      <th>engagement_share_count</th>\n",
       "      <th>engagement_comment_plugin_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6965</th>\n",
       "      <td>6965</td>\n",
       "      <td>business-insider</td>\n",
       "      <td>Business Insider</td>\n",
       "      <td>Monica Chin</td>\n",
       "      <td>The Google Pixel 3 is $350 off at Best Buy — a...</td>\n",
       "      <td>Google's Pixel 3 is an excellent, affordable s...</td>\n",
       "      <td>https://www.businessinsider.com/pixel-3-google...</td>\n",
       "      <td>https://amp.businessinsider.com/images/5d839b4...</td>\n",
       "      <td>2019-09-19T16:22:00Z</td>\n",
       "      <td>It's official: The Google Pixel 4 is coming. G...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7239</th>\n",
       "      <td>7239</td>\n",
       "      <td>newsweek</td>\n",
       "      <td>Newsweek</td>\n",
       "      <td>Jeffery Martin</td>\n",
       "      <td>Obama Jabs Trump for Watching TV, Reading Soci...</td>\n",
       "      <td>At a tech conference, former President Barack ...</td>\n",
       "      <td>https://www.newsweek.com/obama-jabs-trump-watc...</td>\n",
       "      <td>https://d.newsweek.com/en/full/1528639/trump-r...</td>\n",
       "      <td>2019-09-19T03:56:35Z</td>\n",
       "      <td>Speaking at a tech conference Wednesday, forme...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8304</th>\n",
       "      <td>8304</td>\n",
       "      <td>bbc-news</td>\n",
       "      <td>BBC News</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hong Kong protesters hit the streets as China ...</td>\n",
       "      <td>An activist involved in anti-government protes...</td>\n",
       "      <td>https://www.bbc.co.uk/programmes/w172wq4x78lr607</td>\n",
       "      <td>https://ichef.bbci.co.uk/images/ic/1200x675/p0...</td>\n",
       "      <td>2019-10-01T14:31:00Z</td>\n",
       "      <td>An activist involved in anti-government protes...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0         source_id       source_name          author  \\\n",
       "6965        6965  business-insider  Business Insider     Monica Chin   \n",
       "7239        7239          newsweek          Newsweek  Jeffery Martin   \n",
       "8304        8304          bbc-news          BBC News             NaN   \n",
       "\n",
       "                                                  title  \\\n",
       "6965  The Google Pixel 3 is $350 off at Best Buy — a...   \n",
       "7239  Obama Jabs Trump for Watching TV, Reading Soci...   \n",
       "8304  Hong Kong protesters hit the streets as China ...   \n",
       "\n",
       "                                            description  \\\n",
       "6965  Google's Pixel 3 is an excellent, affordable s...   \n",
       "7239  At a tech conference, former President Barack ...   \n",
       "8304  An activist involved in anti-government protes...   \n",
       "\n",
       "                                                    url  \\\n",
       "6965  https://www.businessinsider.com/pixel-3-google...   \n",
       "7239  https://www.newsweek.com/obama-jabs-trump-watc...   \n",
       "8304   https://www.bbc.co.uk/programmes/w172wq4x78lr607   \n",
       "\n",
       "                                           url_to_image          published_at  \\\n",
       "6965  https://amp.businessinsider.com/images/5d839b4...  2019-09-19T16:22:00Z   \n",
       "7239  https://d.newsweek.com/en/full/1528639/trump-r...  2019-09-19T03:56:35Z   \n",
       "8304  https://ichef.bbci.co.uk/images/ic/1200x675/p0...  2019-10-01T14:31:00Z   \n",
       "\n",
       "                                                content  top_article  \\\n",
       "6965  It's official: The Google Pixel 4 is coming. G...          0.0   \n",
       "7239  Speaking at a tech conference Wednesday, forme...          0.0   \n",
       "8304  An activist involved in anti-government protes...          0.0   \n",
       "\n",
       "      engagement_reaction_count  engagement_comment_count  \\\n",
       "6965                        1.0                       1.0   \n",
       "7239                      177.0                      21.0   \n",
       "8304                        0.0                       0.0   \n",
       "\n",
       "      engagement_share_count  engagement_comment_plugin_count  \n",
       "6965                     3.0                              0.0  \n",
       "7239                    18.0                              0.0  \n",
       "8304                     0.0                              0.0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to clean and tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, tokenizer, stopwords):\n",
    "    \"\"\"Pre-process text and generate tokens\n",
    "\n",
    "    Args:\n",
    "        text: Text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        Tokenized text.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()  # Lowercase words\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # Remove [+XYZ chars] in content\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n",
    "    text = re.sub(r\"\\w+…|…\", \"\", text)  # Remove ellipsis (and last word)\n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)  # Replace dash between words\n",
    "    text = re.sub(\n",
    "        f\"[{re.escape(string.punctuation)}]\", \"\", text\n",
    "    )  # Remove punctuation\n",
    "\n",
    "    tokens = tokenizer(text)  # Get tokens from text\n",
    "    tokens = [t for t in tokens if not t in stopwords]  # Remove stopwords\n",
    "    tokens = [\"\" if t.isdigit() else t for t in tokens]  # Remove digits\n",
    "    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply function and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe: (10437, 15)\n",
      "Pre-processed dataframe: (9882, 2)\n"
     ]
    }
   ],
   "source": [
    "custom_stopwords = set(stopwords.words(\"english\") + [\"news\", \"new\", \"top\"])\n",
    "text_columns = [\"title\", \"description\", \"content\"]\n",
    "\n",
    "df = df_raw.copy()\n",
    "df[\"content\"] = df[\"content\"].fillna(\"\")\n",
    "\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "# Create text column based on title, description, and content\n",
    "df[\"text\"] = df[text_columns].apply(lambda x: \" | \".join(x), axis=1)\n",
    "df[\"tokens\"] = df[\"text\"].map(lambda x: clean_text(x, word_tokenize, custom_stopwords))\n",
    "\n",
    "# Remove duplicated after preprocessing\n",
    "_, idx = np.unique(df[\"tokens\"], return_index=True)\n",
    "df = df.iloc[idx, :]\n",
    "\n",
    "# Remove empty values\n",
    "df = df.loc[df.tokens.map(lambda x: len(x) > 0), [\"text\", \"tokens\"]]\n",
    "\n",
    "print(f\"Original dataframe: {df_raw.shape}\")\n",
    "print(f\"Pre-processed dataframe: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df[\"text\"].values\n",
    "tokenized_docs = df[\"tokens\"].values\n",
    "vocab = Counter()\n",
    "for token in tokenized_docs:\n",
    "    vocab.update(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('us', 2757),\n",
       " ('said', 2519),\n",
       " ('year', 1781),\n",
       " ('president', 1756),\n",
       " ('trump', 1705),\n",
       " ('world', 1620),\n",
       " ('says', 1511),\n",
       " ('one', 1418),\n",
       " ('two', 1284),\n",
       " ('first', 1195)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate vectors from document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function for creating a single vectors from word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(list_of_docs, model):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents\n",
    "        model: Gensim's Word Embedding\n",
    "\n",
    "    Returns:\n",
    "        List of document vectors\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply function to previously pre-processed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=tokenized_docs, vector_size=100, workers=1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trumps', 0.988541841506958),\n",
       " ('president', 0.9746493697166443),\n",
       " ('donald', 0.9274922013282776),\n",
       " ('ivanka', 0.9203903079032898),\n",
       " ('impeachment', 0.9195784330368042),\n",
       " ('pences', 0.9152231812477112),\n",
       " ('avlon', 0.9148306846618652),\n",
       " ('biden', 0.9146010279655457),\n",
       " ('breitbart', 0.9144087433815002),\n",
       " ('vice', 0.9067237973213196)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9882, 100)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_docs = vectorize(tokenized_docs, model=model)\n",
    "len(vectorized_docs), len(vectorized_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and analyze clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbkmeans_clusters(X, k, mb=500, print_silhouette_values=False):\n",
    "    \"\"\"Generate clusters.\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches. Defaults to 500.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 50\n",
      "Silhouette coefficient: 0.11\n",
      "Inertia:3561.4632984073837\n",
      "Silhouette values:\n",
      "    Cluster 48: Size:56 | Avg:0.34 | Min:-0.02 | Max: 0.54\n",
      "    Cluster 42: Size:112 | Avg:0.33 | Min:-0.01 | Max: 0.54\n",
      "    Cluster 4: Size:138 | Avg:0.32 | Min:-0.01 | Max: 0.52\n",
      "    Cluster 21: Size:84 | Avg:0.31 | Min:-0.06 | Max: 0.53\n",
      "    Cluster 9: Size:35 | Avg:0.28 | Min:0.03 | Max: 0.52\n",
      "    Cluster 28: Size:132 | Avg:0.27 | Min:-0.09 | Max: 0.52\n",
      "    Cluster 12: Size:253 | Avg:0.26 | Min:-0.00 | Max: 0.47\n",
      "    Cluster 26: Size:60 | Avg:0.26 | Min:-0.04 | Max: 0.51\n",
      "    Cluster 30: Size:122 | Avg:0.25 | Min:-0.06 | Max: 0.46\n",
      "    Cluster 0: Size:122 | Avg:0.25 | Min:-0.02 | Max: 0.47\n",
      "    Cluster 40: Size:107 | Avg:0.23 | Min:-0.07 | Max: 0.48\n",
      "    Cluster 20: Size:140 | Avg:0.23 | Min:-0.08 | Max: 0.46\n",
      "    Cluster 7: Size:182 | Avg:0.19 | Min:-0.03 | Max: 0.39\n",
      "    Cluster 45: Size:57 | Avg:0.18 | Min:-0.03 | Max: 0.41\n",
      "    Cluster 33: Size:181 | Avg:0.17 | Min:0.01 | Max: 0.38\n",
      "    Cluster 22: Size:506 | Avg:0.16 | Min:-0.03 | Max: 0.35\n",
      "    Cluster 14: Size:234 | Avg:0.16 | Min:-0.03 | Max: 0.37\n",
      "    Cluster 25: Size:115 | Avg:0.13 | Min:-0.04 | Max: 0.35\n",
      "    Cluster 46: Size:77 | Avg:0.13 | Min:-0.08 | Max: 0.33\n",
      "    Cluster 24: Size:260 | Avg:0.13 | Min:-0.11 | Max: 0.35\n",
      "    Cluster 15: Size:161 | Avg:0.12 | Min:-0.12 | Max: 0.38\n",
      "    Cluster 10: Size:100 | Avg:0.12 | Min:-0.06 | Max: 0.35\n",
      "    Cluster 43: Size:244 | Avg:0.12 | Min:-0.11 | Max: 0.36\n",
      "    Cluster 37: Size:161 | Avg:0.11 | Min:-0.14 | Max: 0.33\n",
      "    Cluster 11: Size:180 | Avg:0.11 | Min:-0.12 | Max: 0.29\n",
      "    Cluster 19: Size:189 | Avg:0.11 | Min:-0.10 | Max: 0.31\n",
      "    Cluster 41: Size:154 | Avg:0.10 | Min:-0.09 | Max: 0.33\n",
      "    Cluster 34: Size:298 | Avg:0.10 | Min:-0.10 | Max: 0.31\n",
      "    Cluster 16: Size:542 | Avg:0.10 | Min:-0.09 | Max: 0.31\n",
      "    Cluster 23: Size:156 | Avg:0.09 | Min:-0.14 | Max: 0.35\n",
      "    Cluster 32: Size:290 | Avg:0.09 | Min:-0.06 | Max: 0.27\n",
      "    Cluster 18: Size:211 | Avg:0.09 | Min:-0.12 | Max: 0.29\n",
      "    Cluster 36: Size:359 | Avg:0.09 | Min:-0.12 | Max: 0.30\n",
      "    Cluster 47: Size:317 | Avg:0.08 | Min:-0.08 | Max: 0.30\n",
      "    Cluster 31: Size:163 | Avg:0.08 | Min:-0.17 | Max: 0.32\n",
      "    Cluster 8: Size:45 | Avg:0.08 | Min:-0.26 | Max: 0.43\n",
      "    Cluster 3: Size:220 | Avg:0.07 | Min:-0.10 | Max: 0.27\n",
      "    Cluster 13: Size:466 | Avg:0.07 | Min:-0.17 | Max: 0.31\n",
      "    Cluster 39: Size:438 | Avg:0.07 | Min:-0.15 | Max: 0.29\n",
      "    Cluster 29: Size:303 | Avg:0.05 | Min:-0.10 | Max: 0.25\n",
      "    Cluster 35: Size:150 | Avg:0.05 | Min:-0.16 | Max: 0.29\n",
      "    Cluster 6: Size:76 | Avg:0.04 | Min:-0.12 | Max: 0.22\n",
      "    Cluster 27: Size:187 | Avg:0.04 | Min:-0.17 | Max: 0.27\n",
      "    Cluster 49: Size:204 | Avg:0.04 | Min:-0.15 | Max: 0.24\n",
      "    Cluster 38: Size:172 | Avg:0.03 | Min:-0.17 | Max: 0.30\n",
      "    Cluster 1: Size:263 | Avg:0.03 | Min:-0.19 | Max: 0.21\n",
      "    Cluster 2: Size:207 | Avg:0.03 | Min:-0.15 | Max: 0.27\n",
      "    Cluster 44: Size:87 | Avg:0.01 | Min:-0.16 | Max: 0.20\n",
      "    Cluster 5: Size:322 | Avg:-0.00 | Min:-0.21 | Max: 0.22\n",
      "    Cluster 17: Size:244 | Avg:-0.01 | Min:-0.23 | Max: 0.20\n"
     ]
    }
   ],
   "source": [
    "clustering, cluster_labels = mbkmeans_clusters(X=vectorized_docs, k=50, print_silhouette_values=True)\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": docs,\n",
    "    \"tokens\": [\" \".join(text) for text in tokenized_docs],\n",
    "    \"cluster\": cluster_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster (based on centroids):\n",
      "Cluster 0: serial trying shocked contained passenger \n",
      "Cluster 1: rare mosquito train borne commercial \n",
      "Cluster 2: arrest funeral illinois dissident founding \n",
      "Cluster 3: kiev departments spokeswoman repeal saikawa \n",
      "Cluster 4: pm proposals johnsons delay benjamin \n",
      "Cluster 5: december plunged analysts total spring \n",
      "Cluster 6: aides senate congressional pelosi request \n",
      "Cluster 7: speech referendum labour donohoe leo \n",
      "Cluster 8: lilinow path heavy survivors projected \n",
      "Cluster 9: doonbeg disagreed macron emmanuel administrations \n",
      "Cluster 10: stabbing amber neighbor botham guyger \n",
      "Cluster 11: winning takes injury fifth points \n",
      "Cluster 12: likes popularity ai tips access \n",
      "Cluster 13: represents truly shareholders laid planning \n",
      "Cluster 14: zelensky volodymyr whistleblowers ukrainian impeach \n",
      "Cluster 15: apartment murdering suspicion fatal girl \n",
      "Cluster 16: prize throughout tops raw pittsburgh \n",
      "Cluster 17: madrid coaches bulgaria zhengzhou anfield \n",
      "Cluster 18: publicly von discussed conversation concerned \n",
      "Cluster 19: nightclub century collision 20th photographers \n",
      "Cluster 20: squad qualifying warm finals foursomes \n",
      "Cluster 21: tanker arabian yemen ablaze facilities \n",
      "Cluster 22: guests size cutting tale quarterback \n",
      "Cluster 23: soldiers dozens kills bomb victims \n",
      "Cluster 24: parties warned occupied dup suspend \n",
      "Cluster 25: knife headquarters pleaded attacked soldier \n",
      "Cluster 26: cnns interested transcript ridiculed contributor \n",
      "Cluster 27: traded weak firms delivery wireless \n",
      "Cluster 28: tornadoes islands coastal outer floridas \n",
      "Cluster 29: bowl winner wickets minutes scored \n",
      "Cluster 30: cnnpolitics clinton putin complaint zelensky \n",
      "Cluster 31: flows lift slap tariff aimed \n",
      "Cluster 32: tweet insults cnnin panel strategist \n",
      "Cluster 33: intended architecture engage birth respect \n",
      "Cluster 34: selection gardaí suddenly clinic bites \n",
      "Cluster 35: popular feature ios brand netflix \n",
      "Cluster 36: operations illegally drug immigrants espionage \n",
      "Cluster 37: dealt richard zimbabweans ms australian \n",
      "Cluster 38: appearances £50000 mcavoy another april \n",
      "Cluster 39: product success usually fitness turning \n",
      "Cluster 40: clearing unrest defied demonstrators protesters \n",
      "Cluster 41: glasgow beds midlands cctv craig \n",
      "Cluster 42: category humberto landfall charleston floodwaters \n",
      "Cluster 43: sexually daughter accident courtroom pregnant \n",
      "Cluster 44: moscow treasury russian mnuchin urging \n",
      "Cluster 45: centrifuges expressed opinions message desire \n",
      "Cluster 46: impose assembly escalation uaw geneva \n",
      "Cluster 47: rome planned automaker delhi kuala \n",
      "Cluster 48: noaa sharpie forecasters claim assertions \n",
      "Cluster 49: worked testimony angry baltimore notice \n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster (based on centroids):\")\n",
    "for i in range(50):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=5)\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t[0]} \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dorian, Comey and Debra Messing: What Trump tweeted on Labor Day weekend | President Donald Trump axed his visit to Poland over the weekend to monitor Hurricane Dorian from Camp David with emergency management staff, but if the President's more than 120 tweets are any indication, he had more than just the storm on his mind. | Washington (CNN)President Donald Trump axed his visit to Poland over the weekend to monitor Hurricane Dorian from Camp David with emergency management staff, but if the President's more than 120 tweets are any indication, he had more than just the storm on hi… [+3027 chars]\n",
      "-------------\n",
      "Lead NOAA scientist vows to probe agency's defense of Trump | The Acting Head of NOAA, the government weather agency, goes to Alabama Tuesday to address a recent uproar over Hurricane Dorian. President Trump repeatedly and wrongly claimed the hurricane was a threat to Alabama. NOAA was criticized for saying one of its f… | \n",
      "-------------\n",
      "Trump shows ‘altered’ hurricane map after warning of threat to Alabama | Map features pen-drawn loop extending Dorian’s projected path into US state despite weather service declaring no threat to it | Donald Trump showed a map of Hurricane Dorians projected path on Wednesday that appeared to have been altered with a pen to include the state of Alabama.\r\n",
      "The ÜS president in a weekend tweet had named Alabama as one of the states that could be hit. The Nation… [+1630 chars]\n",
      "-------------\n",
      "James Comey Suggests Trump is a Narcissist for Doubling Down on Claim Hurricane Dorian Would Hit Alabama | \"Americans are in harm's way and the president is laser-focused on ... covering up a small mistake he made,\" the former FBI director wrote. | Former FBI Director James Comey on Thursday accused President Donald Trump of deploying narcissism by continuing to claim that official storm projections showed Hurricane Dorian would hit parts of Alabama despite forecasts saying otherwise.\r\n",
      "\"Americans are in… [+2317 chars]\n",
      "-------------\n",
      "MSNBC Contributor Blasts Trump's Bahamas' Decision: 'Extraordinary Measures' to Block 'Specific Skin Color' from U.S. | The Trump administration is reportedly blocking citizens of the Bahamas from being granted temporary protective status following the devastation of Hurricane Dorian. | MSNBC contributor Mike Barnacle blasted President Donald Trump and his administration after it was reported that individuals from the Bahamas displaced by the devastation of Hurricane Dorian would not be granted temporary protected status in the United States… [+2675 chars]\n",
      "-------------\n",
      "CNN's Anderson Cooper Says Trump Is Too Busy Golfing and Tweeting Critics to Follow Hurricane Dorian: 'He's Monitoring the Debra Messing Situation' | The president canceled a trip to Poland to stay behind and monitor the storm, but has been engaging in personal Twitter spats and golfing in Virginia. | CNN host Anderson Cooper has criticized President Donald Trump for a lack of leadership while Hurricane Dorian bears down on the east coast, suggesting the commander in chief is more worried about personal spats than the storm that has prompted evacuation ord… [+3222 chars]\n",
      "-------------\n",
      "Ross Must Resign If Report He Threatened NOAA Officials Is True: Democrat | As President Donald Trump claimed Hurricane Dorian could hit Alabama, the National Weather Service tweeted to correct the rumors. | Commerce Secretary Wilbur Ross is facing calls to resign over a report alleging that he threatened to fire top officials at NOAA for a tweet disputing President Donald Trump's claim that Hurricane Dorian would hit Alabama.\r\n",
      "\"If that story is true, and I don't… [+3828 chars]\n",
      "-------------\n",
      "Federal weather workers are furious at the NOAA's 'utterly disgusting' statement defending Trump's claim Hurricane Dorian would hit Alabama | Federal weather workers have reacted furiously to the National Oceanic and Atmospheric Administration's (NOAA) defence of US President Donald Trump's repeated assertions that Hurricane Dorian was set to hit Alabama. \"Never ever before has their management thr… | Federal weather workers have reacted furiously to the National Oceanic and Atmospheric Administration's (NOAA) defence of US President Donald Trump's repeated assertions that Hurricane Dorian was set to hit Alabama, saying they have been \"thrown under the bus… [+3510 chars]\n",
      "-------------\n",
      "GOP Senator Marco Rubio Says It Has Always Been 'Unlawful Immigration' to the Bahamas That Has Been a Problem—Not the Other Way Around | Florida Republican Senator Marco Rubio's comments came as President Donald Trump demanded all Bahamians fleeing the devastation of Hurricane Dorian have \"totally proper documentation\" before trying to come to the U.S. | As President Donald Trump doubled down on demands that Bahamians fleeing the destruction of Hurricane Dorian have \"totally proper documentation\" in order to come to the U.S., Republican Senator Marco Rubio of Florida, the state where many Bahamians have been … [+2310 chars]\n",
      "-------------\n",
      "Trump's feud with weather officials over Alabama tweet ramps up | The National Oceanic and Atmospheric Administration issued a statement Friday to say the information they gave the president over the weekend did in fact include Alabama in their Hurricane Dorian projections. The statement also disavowed a Sunday tweet sent o… | \n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "test_cluster = 48\n",
    "most_representative_docs = np.argsort(\n",
    "    np.linalg.norm(vectorized_docs - clustering.cluster_centers_[test_cluster], axis=1)\n",
    ")\n",
    "for d in most_representative_docs[:10]:\n",
    "    print(docs[d])\n",
    "    print(\"-------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
