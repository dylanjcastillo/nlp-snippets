{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries and define utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_text(sample, clean):\n",
    "    print(f\"Before: {sample}\")\n",
    "    print(f\"After: {clean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"THIS TEXT WILL BE LOWERCASED. THIS WON'T: ÃŸÃŸÃŸ\"\n",
    "clean_text = sample_text.lower()\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: THIS TEXT WILL BE LOWERCASED. THIS WON'T: ÃŸÃŸÃŸ\n",
    "# After: this text will be lowercased. this won't: ÃŸÃŸÃŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove cases (useful for caseles matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"THIS TEXT WILL BE LOWERCASED. THIS too: ÃŸÃŸÃŸ\"\n",
    "clean_text = sample_text.casefold()\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: THIS TEXT WILL BE LOWERCASED. THIS too: ÃŸÃŸÃŸ\n",
    "# After: this text will be lowercased. this too: ssssss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Some URLs: https://example.com http://example.io http://exam-ple.com More text\"\n",
    "clean_text = re.sub(r\"https?://\\S+\", \"\", sample_text)\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: Some URLs: https://example.com http://example.io http://exam-ple.com More text\n",
    "# After: Some URLs:    More text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove \\<a\\> tags but keep its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Here's <a href='https://example.com'> a tag</a>\"\n",
    "clean_text = re.sub(r\"<a[^>]*>(.*?)</a>\", r\"\\1\", sample_text)\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: Here's <a href='https://example.com'> a tag</a>\n",
    "# After: Here's  a tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "<body>\n",
    "<div> This is a sample text with <b>lots of tags</b> </div>\n",
    "<br/>\n",
    "</body>\n",
    "\"\"\"\n",
    "clean_text = re.sub(r\"<.*?>\", \"\", sample_text)\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: \n",
    "# <body>\n",
    "# <div> This is a sample text with <b>lots of tags</b> </div>\n",
    "# <br/>\n",
    "# </body>\n",
    "\n",
    "# After: \n",
    "\n",
    "#  This is a sample text with lots of tags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove multiple spaces, tabs, and indents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"     \\t\\tA      text\\t\\t\\t\\n\\n sample       \"\n",
    "clean_text = \" \".join(sample_text.split())\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before:      \t\tA      text\t\t\t\n",
    "\n",
    "#  sample       \n",
    "# After: A text sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "sample_text = \"A lot of !!!! .... ,,,, ;;;;;;;?????\"\n",
    "clean_text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", sample_text)\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: A lot of !!!! .... ,,,, ;;;;;;;?????\n",
    "# After: A lot of   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"These are some numbers: 1919191 2229292 11.233 22/22/22. But don't remove this one H2O\"\n",
    "clean_text = re.sub(r\"\\b[0-9]+\\b\\s*\", \"\", sample_text)\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: These are some numbers: 1919191 2229292 11.233 22/22/22. But don't remove this one H2O\n",
    "# After: These are some numbers: .//. But don't remove this one H2O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"I want to keep this one: 10/10/20 but not this one 222333\"\n",
    "clean_text = \" \".join([w for w in sample_text.split() if not w.isdigit()]) # Side effect: removes extra spaces\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: I want to keep this one: 10/10/20 but not this one 222333\n",
    "# After: I want to keep this one: 10/10/20 but not this one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove non-alphabetic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Sample text with numbers 123455 and words\"\n",
    "clean_text = \" \".join([w for w in sample_text.split() if w.isalpha()]) # Side effect: removes extra spaces\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: Sample text with numbers 123455 and words\n",
    "# After: Sample text with numbers and words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove non-alphanumeric characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Sample text 123 !!!! Haha.... !!!! ##$$$%%%%\"\n",
    "clean_text = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", sample_text)\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: Sample text 123 !!!! Haha.... !!!! ##$$$%%%%\n",
    "# After: Sample text 123  Haha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"is\", \"a\"]\n",
    "sample_text = \"this is a sample text\"\n",
    "tokens = sample_text.split()\n",
    "clean_tokens = [t for t in tokens if not t in stopwords]\n",
    "clean_text = \" \".join(clean_tokens)\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: this is a sample text\n",
    "# After: this sample text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove short tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"this is a sample text. I'll remove the a\"\n",
    "tokens = sample_text.split()\n",
    "clean_tokens = [t for t in tokens if len(t) > 1]\n",
    "clean_text = \" \".join(clean_tokens)\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: this is a sample text. I'll remove the a\n",
    "# After: this is sample text. I'll remove the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform emojis to characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emoji import demojize\n",
    "\n",
    "sample_text = \"I love ðŸ¥‘\"\n",
    "clean_text = demojize(sample_text)\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: I love ðŸ¥‘\n",
    "# After: I love :avocado:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sample_text = \"this is a text ready to tokenize\"\n",
    "tokens = word_tokenize(sample_text)\n",
    "print_text(sample_text, tokens)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: this is a text ready to tokenize\n",
    "# After: ['this', 'is', 'a', 'text', 'ready', 'to', 'tokenize']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize tweets using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "sample_text = \"This is a tweet @jack #NLP\"\n",
    "tokens = tweet_tokenizer.tokenize(sample_text)\n",
    "print_text(sample_text, tokens)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: This is a tweet @jack #NLP\n",
    "# After: ['This', 'is', 'a', 'tweet', '@jack', '#NLP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split text into sentences using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sample_text = \"This is a sentence. This is another one!\\nAnd this is the last one.\"\n",
    "sentences = sent_tokenize(sample_text)\n",
    "print_text(sample_text, sentences)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: This is a sentence. This is another one!\n",
    "# And this is the last one.\n",
    "# After: ['This is a sentence.', 'This is another one!', 'And this is the last one.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sample_text = \"this is a text ready to tokenize\"\n",
    "doc = nlp(sample_text)\n",
    "tokens = [token.text for token in doc]\n",
    "print_text(sample_text, tokens)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: this is a text ready to tokenize\n",
    "# After: ['this', 'is', 'a', 'text', 'ready', 'to', 'tokenize']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split text into sentences using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sample_text = \"This is a sentence. This is another one!\\nAnd this is the last one.\"\n",
    "doc = nlp(sample_text)\n",
    "sentences = [sentence.text for sentence in doc.sents]\n",
    "print_text(sample_text, sentences)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: This is a sentence. This is another one!\n",
    "# And this is the last one.\n",
    "# After: ['This is a sentence.', 'This is another one!\\n', 'And this is the last one.']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
