{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd recommend you to combine the snippets you need into a function. Then, you can use that function for pre-processing or tokenizing text. If you're using **pandas** you can apply that function to a specific column using the `.map` method of pandas' `Series`. \n",
    "\n",
    "Take a look at the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        this text needs some cleaning\n",
       "1                        this text too\n",
       "2    yes you got it right this one too\n",
       "Name: text_col, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"text_col\": [\n",
    "        \"This TEXT needs \\t\\t\\tsome cleaning!!!...\", \n",
    "        \"This text too!!...       \", \n",
    "        \"Yes, you got it right!\\n This one too\\n\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase text\n",
    "    text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", text)  # Remove punctuation\n",
    "    text = \" \".join(text.split())  # Remove extra spaces, tabs, and new lines\n",
    "    return text\n",
    "\n",
    "df[\"text_col\"].map(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're planning on testing these snippets on your own, make sure to copy the following function at the top of your Python script or Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_text(sample, clean):\n",
    "    print(f\"Before: {sample}\")\n",
    "    print(f\"After: {clean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercase text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"THIS TEXT WILL BE LOWERCASED. THIS WON'T: ßßß\"\n",
    "clean_text = sample_text.lower()\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: THIS TEXT WILL BE LOWERCASED. THIS WON'T: ßßß\n",
    "# After: this text will be lowercased. this won't: ßßß"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove cases (useful for caseles matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"THIS TEXT WILL BE LOWERCASED. THIS too: ßßß\"\n",
    "clean_text = sample_text.casefold()\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: THIS TEXT WILL BE LOWERCASED. THIS too: ßßß\n",
    "# After: this text will be lowercased. this too: ssssss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sample_text = \"Some URLs: https://example.com http://example.io http://exam-ple.com More text\"\n",
    "clean_text = re.sub(r\"https?://\\S+\", \"\", sample_text)\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: Some URLs: https://example.com http://example.io http://exam-ple.com More text\n",
    "# After: Some URLs:    More text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove \\<a\\> tags but keep its content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sample_text = \"Here's <a href='https://example.com'> a tag</a>\"\n",
    "clean_text = re.sub(r\"<a[^>]*>(.*?)</a>\", r\"\\1\", sample_text)\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: Here's <a href='https://example.com'> a tag</a>\n",
    "# After: Here's  a tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "<body>\n",
      "<div> This is a sample text with <b>lots of tags</b> </div>\n",
      "<br/>\n",
      "</body>\n",
      "\n",
      "After: \n",
      " \n",
      "  This is a sample text with  lots of tags   \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sample_text = \"\"\"\n",
    "<body>\n",
    "<div> This is a sample text with <b>lots of tags</b> </div>\n",
    "<br/>\n",
    "</body>\n",
    "\"\"\"\n",
    "clean_text = re.sub(r\"<.*?>\", \" \", sample_text)\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: \n",
    "# <body>\n",
    "# <div> This is a sample text with <b>lots of tags</b> </div>\n",
    "# <br/>\n",
    "# </body>\n",
    "\n",
    "# After: \n",
    "\n",
    "#  This is a sample text with lots of tags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove extra spaces, tabs, and line breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"     \\t\\tA      text\\t\\t\\t\\n\\n sample       \"\n",
    "clean_text = \" \".join(sample_text.split())\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before:      \t\tA      text\t\t\t\n",
    "\n",
    "#  sample       \n",
    "# After: A text sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "sample_text = \"A lot of !!!! .... ,,,, ;;;;;;;?????\"\n",
    "clean_text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", sample_text)\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: A lot of !!!! .... ,,,, ;;;;;;;?????\n",
    "# After: A lot of   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sample_text = \"Remove these numbers: 1919191 2229292 11.233 22/22/22. But don't remove this one H2O\"\n",
    "clean_text = re.sub(r\"\\b[0-9]+\\b\\s*\", \"\", sample_text)\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: Remove these numbers: 1919191 2229292 11.233 22/22/22. But don't remove this one H2O\n",
    "# After: Remove these numbers: .//. But don't remove this one H2O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"I want to keep this one: 10/10/20 but not this one 222333\"\n",
    "clean_text = \" \".join([w for w in sample_text.split() if not w.isdigit()]) # Side effect: removes extra spaces\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: I want to keep this one: 10/10/20 but not this one 222333\n",
    "# After: I want to keep this one: 10/10/20 but not this one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove non-alphabetic characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Sample text with numbers 123455 and words\"\n",
    "clean_text = \" \".join([w for w in sample_text.split() if w.isalpha()]) # Side effect: removes extra spaces\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: Sample text with numbers 123455 and words\n",
    "# After: Sample text with numbers and words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove all special characters and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sample_text = \"Sample text 123 !!!! Haha.... !!!! ##$$$%%%%\"\n",
    "clean_text = re.sub(r\"[^A-Za-z0-9\\s]+\", \"\", sample_text)\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: Sample text 123 !!!! Haha.... !!!! ##$$$%%%%\n",
    "# After: Sample text 123  Haha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"is\", \"a\"]\n",
    "sample_text = \"this is a sample text\"\n",
    "tokens = sample_text.split()\n",
    "clean_tokens = [t for t in tokens if not t in stopwords]\n",
    "clean_text = \" \".join(clean_tokens)\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: this is a sample text\n",
    "# After: this sample text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove short tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"this is a sample text. I'll remove the a\"\n",
    "tokens = sample_text.split()\n",
    "clean_tokens = [t for t in tokens if len(t) > 1]\n",
    "clean_text = \" \".join(clean_tokens)\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: this is a sample text. I'll remove the a\n",
    "# After: this is sample text. I'll remove the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform emojis to characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emoji import demojize\n",
    "\n",
    "sample_text = \"I love 🥑\"\n",
    "clean_text = demojize(sample_text)\n",
    "print_text(sample_text, clean_text)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: I love 🥑\n",
    "# After: I love :avocado:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the NLTK's snippets, you need to install NLTK. You can do that as follows: `pip install nltk`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize text using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sample_text = \"this is a text ready to tokenize\"\n",
    "tokens = word_tokenize(sample_text)\n",
    "print_text(sample_text, tokens)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: this is a text ready to tokenize\n",
    "# After: ['this', 'is', 'a', 'text', 'ready', 'to', 'tokenize']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize tweets using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "sample_text = \"This is a tweet @jack #NLP\"\n",
    "tokens = tweet_tokenizer.tokenize(sample_text)\n",
    "print_text(sample_text, tokens)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: This is a tweet @jack #NLP\n",
    "# After: ['This', 'is', 'a', 'tweet', '@jack', '#NLP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split text into sentences using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sample_text = \"This is a sentence. This is another one!\\nAnd this is the last one.\"\n",
    "sentences = sent_tokenize(sample_text)\n",
    "print_text(sample_text, sentences)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: This is a sentence. This is another one!\n",
    "# And this is the last one.\n",
    "# After: ['This is a sentence.', 'This is another one!', 'And this is the last one.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the spaCy's snippets, you need to install the library as follows: `pip install spacy`. You also need to download a language model. For English, here's how you do it: `python -m spacy download en_core_web_sm`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize text using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sample_text = \"this is a text ready to tokenize\"\n",
    "doc = nlp(sample_text)\n",
    "tokens = [token.text for token in doc]\n",
    "print_text(sample_text, tokens)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: this is a text ready to tokenize\n",
    "# After: ['this', 'is', 'a', 'text', 'ready', 'to', 'tokenize']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split text into sentences using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sample_text = \"This is a sentence. This is another one!\\nAnd this is the last one.\"\n",
    "doc = nlp(sample_text)\n",
    "sentences = [sentence.text for sentence in doc.sents]\n",
    "print_text(sample_text, sentences)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: This is a sentence. This is another one!\n",
    "# And this is the last one.\n",
    "# After: ['This is a sentence.', 'This is another one!\\n', 'And this is the last one.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize text using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "sample_text = 'This is a text you want to tokenize using KERAS!!'\n",
    "tokens = text_to_word_sequence(sample_text)\n",
    "print_text(sample_text, tokens)\n",
    "\n",
    "# ----- Expected output -----\n",
    "# Before: This is a text you want to tokenize using KERAS!!\n",
    "# After: ['this', 'is', 'a', 'text', 'you', 'want', 'to', 'tokenize', 'using', 'keras']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
